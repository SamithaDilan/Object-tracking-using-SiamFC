{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiamFC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdrq2fWJVTG5",
        "outputId": "94b45735-502e-4282-85ac-3aa30a279a61"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6U9rw-859cn",
        "outputId": "00f4ef13-3998-4d6d-aef8-76e08ec35fd1"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uk_LG4kA8cW"
      },
      "source": [
        "class Config:\n",
        "    # dataset related\n",
        "    exemplar_size = 127                    # exemplar size TEST\n",
        "    instance_size = 255                    # instance size\n",
        "    context_amount = 0.5                   # context amount\n",
        "    scale_num = 3                          # the number of scale\n",
        "    gpu_id = 0                             # gpu_id\n",
        "    # training related\n",
        "    train_per_epoch = 4880                 # num of samples per epoch\n",
        "    val_per_epoch = 520                    \n",
        "    frame_range = 100                      # frame range of choosing the instance\n",
        "    train_batch_size = 8                   # training batch size\n",
        "    valid_batch_size = 8                   # validation batch size\n",
        "    train_num_workers = 8                  # number of workers of train dataloader\n",
        "    valid_num_workers = 8                  # number of workers of validation dataloader\n",
        "    lr = 1e-2                              # learning rate of SGD 0.01\n",
        "    step_size = 25                         # step size of LR_Schedular\n",
        "    gamma = 0.1                            # decay rate of LR_Schedular\n",
        "    epoch = 30                             # total epoch\n",
        "    seed = 1234                            # seed to sample training videos\n",
        "    radius = 16                            # radius of positive label\n",
        "    response_scale = 1e-3                  # normalize of response\n",
        "    max_translate = 3                      # max translation of random shift\n",
        "    momentum = 0.9                         # momentum of SGD\n",
        "    weight_decay = 1e-4                    # weight decay of optimizator                      \n",
        "\n",
        "    # tracking related\n",
        "    train_response_sz = 17                 # train response size\n",
        "    scale_step = 1.0375                    # scale step of instance image\n",
        "    num_scale = 3                          # number of scales\n",
        "    scale_lr = 0.59                        # scale learning rate\n",
        "    response_up_stride = 16                # response upsample stride\n",
        "    response_sz = 17                       # response size\n",
        "    window_influence = 0.25                # window influence\n",
        "    scale_penalty = 0.9745                 # scale penalty\n",
        "    total_stride = 8                       # total stride of backbone\n",
        "    gray_ratio = 0.25\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JX81Zvk_xP4"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from concurrent import futures\n",
        "\n",
        "datapath = '/content/drive/MyDrive/Colab Notebooks/validation'\n",
        "outputpath = '/content/drive/MyDrive/Colab Notebooks/val_crop_'\n",
        "num_processings = None\n",
        "cfg = config\n",
        "assert 1 == 2\n",
        "def _init_video(video):\n",
        "\n",
        "    frame_name_list = glob.glob(os.path.join(datapath, video)+'/**.jpg')\n",
        "    frame_name_list.sort()\n",
        "\n",
        "    try:\n",
        "        gt_file = os.path.join(datapath, video, 'groundtruth.txt')\n",
        "        try:\n",
        "            gt = np.loadtxt(gt_file, dtype=float, delimiter=',')\n",
        "        except:\n",
        "            gt = np.loadtxt(gt_file, dtype=float)\n",
        "    except:\n",
        "        gt_file = os.path.join(datapath, video, 'groundtruth_rect.txt')\n",
        "        try:\n",
        "            gt = np.loadtxt(gt_file, dtype=float, delimiter=',')\n",
        "        except:\n",
        "            gt = np.loadtxt(gt_file, dtype=float)\n",
        "\n",
        "    n_frames = len(frame_name_list)\n",
        "    assert n_frames == len(gt), 'Number of frames and number of GT lines should be equal.'\n",
        "    return gt, frame_name_list, n_frames\n",
        "\n",
        "def get_center(x):\n",
        "    return (x - 1.) / 2.\n",
        "\n",
        "def xyxy2cxcywh(bbox):\n",
        "    return get_center(bbox[0]+bbox[2]), get_center(bbox[1]+bbox[3]), (bbox[2]-bbox[0]), (bbox[3]-bbox[1])\n",
        "\n",
        "def crop_and_pad(img, cx, cy, model_sz, original_sz, img_mean=None):\n",
        "    xmin = cx - original_sz // 2\n",
        "    xmax = cx + original_sz // 2\n",
        "    ymin = cy - original_sz // 2\n",
        "    ymax = cy + original_sz // 2\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    left = right = top = bottom = 0\n",
        "    if xmin < 0:\n",
        "        left = int(abs(xmin))\n",
        "    if xmax > im_w:\n",
        "        right = int(xmax - im_w)\n",
        "    if ymin < 0:\n",
        "        top = int(abs(ymin))\n",
        "    if ymax > im_h:\n",
        "        bottom = int(ymax - im_h)\n",
        "\n",
        "    xmin = int(max(0, xmin))\n",
        "    xmax = int(min(im_w, xmax))\n",
        "    ymin = int(max(0, ymin))\n",
        "    ymax = int(min(im_h, ymax))\n",
        "    im_patch = img[ymin:ymax, xmin:xmax]\n",
        "    if left != 0 or right !=0 or top!=0 or bottom!=0:\n",
        "        if img_mean is None:\n",
        "            img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n",
        "        im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n",
        "                cv2.BORDER_CONSTANT, value=img_mean)\n",
        "    if model_sz != original_sz:\n",
        "        im_patch = cv2.resize(im_patch, (model_sz, model_sz))\n",
        "    return im_patch\n",
        "\n",
        "def get_instance_image(img, bbox, size_z, size_x, context_amount, img_mean=None):\n",
        "    cx, cy, w, h = xyxy2cxcywh(bbox)\n",
        "    wc_z = w + context_amount * (w+h)\n",
        "    hc_z = h + context_amount * (w+h)\n",
        "    s_z = np.sqrt(wc_z * hc_z)\n",
        "    scale_z = size_z / s_z\n",
        "    d_search = (size_x - size_z) / 2\n",
        "    pad = d_search / scale_z\n",
        "    s_x = s_z + 2 * pad\n",
        "    scale_x = size_x / s_x\n",
        "    instance_img = crop_and_pad(img, cx, cy, size_x, s_x, img_mean)\n",
        "    return instance_img, scale_x, s_x\n",
        "\n",
        "def crop_video(video):\n",
        "\n",
        "    gt, frame_name_list, n_frames = _init_video(video)\n",
        "    savepath = os.path.join(outputpath, video)\n",
        "    if not os.path.exists(savepath):\n",
        "        os.mkdir(savepath)\n",
        "    for i in range(n_frames):\n",
        "        img = cv2.imread(frame_name_list[i])\n",
        "        img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n",
        "        bbox = gt[i]\n",
        "        bbox = [int(bbox[0]), int(bbox[1]), int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3])]\n",
        "        instance_img, _, _ = get_instance_image(img, bbox,\n",
        "                                                cfg.exemplar_size, cfg.instance_size, cfg.context_amount,\n",
        "                                                img_mean)\n",
        "        cv2.imwrite(savepath+\"/{:0>8d}.x.jpg\".format(i+1), instance_img)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    videos = os.listdir(datapath)\n",
        "    videos.sort()\n",
        "    for video in videos:\n",
        "        crop_video(video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hezRncRKRi1C"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, input_channel: int, total_stride: int):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, input_channel // total_stride, 1, padding=0, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(input_channel // total_stride, input_channel, 1, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        feat_avg = self.mlp(self.avg_pool(x))\n",
        "        feat_max = self.mlp(self.max_pool(x))\n",
        "\n",
        "        return self.sigmoid(feat_avg + feat_max)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Conv2d(2, 1, 7, padding=3, bias=False)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        max_feature = torch.max(x, dim=1, keepdim=True)[0]\n",
        "        avg_feature = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        feature_map = torch.cat((feat_avg, feat_max), dim=1)\n",
        "\n",
        "        return self.sigmoid(self.conv_layer(feature_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hGahvgC4yHr"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SiamFCNet(nn.Module):\n",
        "    def __init__(self, training=True):\n",
        "        super(SiamFCNet,self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3,96,11,2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3,2),\n",
        "            # nn.Dropout(p=0.02)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96,256,5,1, groups=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            # nn.Dropout(p=0.03)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 3, 1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.Dropout(p=0.05)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 192, 3, 1, groups=2),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(192,256,3,1, groups=2)\n",
        "            # nn.Dropout(p=0.20)\n",
        "        )\n",
        "        # self.ca_mod1 = CBAM(96, config.total_stride)\n",
        "        # self.ca_mod2 = CBAM(256, config.total_stride)\n",
        "        # self.ca_mod3 = CBAM(384, config.total_stride)\n",
        "        # self.ca_mod4 = CBAM(192, config.total_stride)\n",
        "        # self.ca_mod5 = CBAM(256, config.total_stride)\n",
        "\n",
        "        self.ca_mod1 = ChannelAttention(96, config.total_stride)\n",
        "        self.sa_mod1 = SpatialAttention()\n",
        "        self.ca_mod2 = ChannelAttention(256, config.total_stride)\n",
        "        self.sa_mod2 = SpatialAttention()\n",
        "        self.ca_mod3 = ChannelAttention(384, config.total_stride)\n",
        "        self.sa_mod3 = SpatialAttention()\n",
        "        self.ca_mod4 = ChannelAttention(192, config.total_stride)\n",
        "        self.sa_mod4 = SpatialAttention()\n",
        "        self.ca_mod5 = ChannelAttention(256, config.total_stride)\n",
        "        self.sa_mod5 = SpatialAttention()\n",
        "\n",
        "        # self.sa = SpatialAttention()\n",
        "\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.bais = nn.Parameter(torch.zeros(1))\n",
        "        self.exemplar = None\n",
        "        self.training = training\n",
        "\n",
        "        if self.training:\n",
        "            #generate label\n",
        "            gt, weight = self.create_label((config.response_sz, config.response_sz))\n",
        "            # Get labels and convert it to GPU\n",
        "            with torch.cuda.device(0):\n",
        "                self.train_gt = torch.from_numpy(gt).cuda()\n",
        "                self.train_weight = torch.from_numpy(weight).cuda()\n",
        "                self.valid_gt = torch.from_numpy(gt).cuda()\n",
        "                self.valid_weight = torch.from_numpy(weight).cuda()\n",
        "            \n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    # def attention_module(self, feature_map):\n",
        "    #     feature_out = self.ca(feature_map) * feature_map\n",
        "    #     feature_out = self.sa(feature_out) * feature_out\n",
        "    #     return self.relu(feature_out + feature_map)\n",
        "\n",
        "    def forward(self, z, x):\n",
        "\n",
        "        if z is not None and x is not None:\n",
        "            feature_z = self.conv1(z)\n",
        "            feature_z_ca = self.ca_mod1(feature_z) * feature_z\n",
        "            # feature_z_ca = self.sa_mod1(feature_z_ca) * feature_z_ca\n",
        "            feature_z = self.relu(feature_z + feature_z_ca)\n",
        "\n",
        "            feature_z = self.conv2(feature_z)\n",
        "            # feature_z_ca = self.ca_mod2(feature_z) * feature_z\n",
        "            # feature_z_ca = self.sa_mod2(feature_z_ca) * feature_z_ca\n",
        "            # feature_z = self.relu(feature_z + feature_z_ca)\n",
        "\n",
        "            feature_z = self.conv3(feature_z)\n",
        "            feature_z_ca = self.ca_mod3(feature_z) * feature_z\n",
        "            # feature_z_ca = self.sa_mod3(feature_z_ca) * feature_z_ca\n",
        "            feature_z = self.relu(feature_z + feature_z_ca)\n",
        "\n",
        "            feature_z = self.conv4(feature_z)\n",
        "            feature_z_ca = self.ca_mod4(feature_z) * feature_z\n",
        "            # feature_z_ca = self.sa_mod4(feature_z_ca) * feature_z_ca\n",
        "            feature_z = self.relu(feature_z + feature_z_ca)\n",
        "\n",
        "            feature_z = self.conv5(feature_z)\n",
        "            # feature_z_ca = self.ca_mod5(feature_z) * feature_z\n",
        "            # feature_z_ca = self.sa_mod5(feature_z_ca) * feature_z_ca\n",
        "            # feature_z = self.relu(feature_z + feature_z_ca) # shape = ([8, 256, 6, 6])\n",
        "\n",
        "            feature_x = self.conv1(x)\n",
        "            feature_x_ca = self.ca_mod1(feature_x) * feature_x\n",
        "            # feature_x_ca = self.sa_mod1(feature_x_ca) * feature_x_ca\n",
        "            feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv2(feature_x)\n",
        "            # feature_x_ca = self.ca_mod2(feature_x) * feature_x\n",
        "            # feature_x_ca = self.sa_mod2(feature_x_ca) * feature_x_ca\n",
        "            # feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv3(feature_x)\n",
        "            feature_x_ca = self.ca_mod3(feature_x) * feature_x\n",
        "            # feature_x_ca = self.sa_mod3(feature_x_ca) * feature_x_ca\n",
        "            feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv4(feature_x)\n",
        "            feature_x_ca = self.ca_mod4(feature_x) * feature_x\n",
        "            # feature_x_ca = self.sa_mod4(feature_x_ca) * feature_x_ca\n",
        "            feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv5(feature_x)\n",
        "            # feature_x_ca = self.ca_mod5(feature_x) * feature_x\n",
        "            # feature_x_ca = self.sa_mod5(feature_x_ca) * feature_x_ca\n",
        "            # feature_x = self.relu(feature_x + feature_x_ca) # shape = ([8, 256, 22, 22])\n",
        "\n",
        "            N = feature_x.shape[0] # N = 8\n",
        "            # reshape tensor by view [1, 2048, 22, 22]\n",
        "            feature_x = feature_x.view(1, -1, feature_x.shape[2], feature_x.shape[3])\n",
        "            score = F.conv2d(feature_x, feature_z, groups=N)*config.response_scale + self.bais\n",
        "            score = score.view(N,-1,score.shape[2],score.shape[3]) # score shape ([8, 1, 17, 17])\n",
        "            return score\n",
        "        elif z is None and x is not None:\n",
        "            feature_x = self.conv1(x)\n",
        "            # feature_x_ca = self.ca_mod1(feature_x) * feature_x\n",
        "            # feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv2(feature_x)\n",
        "\n",
        "            feature_x = self.conv3(feature_x)\n",
        "            # feature_x_ca = self.ca_mod3(feature_x) * feature_x\n",
        "            # feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv4(feature_x)\n",
        "            # feature_x_ca = self.ca_mod4(feature_x) * feature_x\n",
        "            # feature_x = self.relu(feature_x + feature_x_ca)\n",
        "\n",
        "            feature_x = self.conv5(feature_x) # shape = ([3, 256, 22, 22])\n",
        "            \n",
        "            N = feature_x.shape[0] #N = 3\n",
        "            # reshape tensor by view [1, 768, 22, 22]\n",
        "            feature_x = feature_x.view(1, -1, feature_x.shape[2], feature_x.shape[3])\n",
        "            score = F.conv2d(feature_x, self.exemplar, groups=N) * config.response_scale + self.bais\n",
        "            score = score.view(N, -1, score.shape[2], score.shape[3]) # score shape ([3, 1, 17, 17])\n",
        "            return score\n",
        "        else:\n",
        "            self.exemplar = self.conv1(z)\n",
        "            # exemplar_ca = self.ca_mod1(self.exemplar) * self.exemplar\n",
        "            # self.exemplar = self.relu(self.exemplar + exemplar_ca)\n",
        "\n",
        "            self.exemplar = self.conv2(self.exemplar)\n",
        "\n",
        "            self.exemplar = self.conv3(self.exemplar)\n",
        "            # exemplar_ca = self.ca_mod3(self.exemplar) * self.exemplar\n",
        "            # self.exemplar = self.relu(self.exemplar + exemplar_ca)\n",
        "\n",
        "            self.exemplar = self.conv4(self.exemplar)\n",
        "            # exemplar_ca = self.ca_mod4(self.exemplar) * self.exemplar\n",
        "            # self.exemplar = self.relu(self.exemplar + exemplar_ca)\n",
        "\n",
        "            self.exemplar = self.conv5(self.exemplar)\n",
        "            self.exemplar = torch.cat([self.exemplar for _ in range(3)], dim=0)\n",
        "\n",
        "\n",
        "\n",
        "    def loss(self, pred):\n",
        "        cost_function = nn.BCEWithLogitsLoss(weight=self.train_weight, reduction='sum')\n",
        "        loss = cost_function(pred, self.train_gt) #/config.train_batch_size\n",
        "        return loss\n",
        "        \n",
        "    def weighted_loss(self, pred):\n",
        "        if self.training:\n",
        "            return F.binary_cross_entropy_with_logits(pred, self.train_gt,\n",
        "                    self.train_weight, reduction='sum') / config.train_batch_size # normalize the batch_size\n",
        "        else:\n",
        "            return F.binary_cross_entropy_with_logits(pred, self.valid_gt,\n",
        "                    self.valid_weight, reduction='sum') / config.train_batch_size # normalize the batch_size\n",
        "                    \n",
        "    def create_label(self, shape):\n",
        "        h, w = shape # 17 x 17\n",
        "        # Values are generated within the half-open interval [start, stop) \n",
        "        y = np.arange(h, dtype=np.float32) - (h - 1) / 2.\n",
        "        x = np.arange(w, dtype=np.float32) - (w - 1) / 2.\n",
        "        y, x = np.meshgrid(y, x)\n",
        "        dist = np.sqrt(x ** 2 + y ** 2)\n",
        "        label = np.zeros((h, w)) \n",
        "        label[dist <= config.radius / config.total_stride] = 1\n",
        "\n",
        "        #increase the dimension\n",
        "        label = label[np.newaxis, :, :]\n",
        "\n",
        "        #create same shape array with weights\n",
        "        weights = np.ones_like(label)\n",
        "        weights[label == 1] = 0.5 / np.sum(label == 1)\n",
        "        weights[label == 0] = 0.5 / np.sum(label == 0)\n",
        "        label = np.repeat(label, config.train_batch_size, axis=0)[:, np.newaxis, :, :]\n",
        "        return label.astype(np.float32), weights.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vODMzl9Q5Jdk"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import Dataset\n",
        "np.random.seed(2)\n",
        "\n",
        "class Dataset_GOT(Dataset):\n",
        "    def __init__(self, data_dir, z_transforms, x_transforms, training=True):\n",
        "        self.data_dir = data_dir\n",
        "        self.videos = os.listdir(data_dir)\n",
        "        self.z_transforms = z_transforms\n",
        "        self.x_transforms = x_transforms\n",
        "        if training:\n",
        "            self.num = config.train_per_epoch\n",
        "        else:\n",
        "            self.num = config.val_per_epoch\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = index % len(self.videos)\n",
        "        video = self.videos[index]\n",
        "        video_path = os.path.join(self.data_dir, video)\n",
        "        n_frames = len(os.listdir(video_path))\n",
        "        z_id = np.random.choice(n_frames)\n",
        "        z_path = os.path.join(video_path, \"{:0>8d}.x.jpg\".format(z_id+1))\n",
        "        z = cv2.imread(z_path, cv2.IMREAD_COLOR)\n",
        "        z = cv2.cvtColor(z, cv2.COLOR_BGR2RGB)\n",
        "        low_limit = max(0, z_id - config.frame_range)\n",
        "        up_limit = min(n_frames, z_id + config.frame_range)\n",
        "        x_id = np.random.choice(range(low_limit,up_limit))\n",
        "        x_path = os.path.join(video_path, \"{:0>8d}.x.jpg\".format(x_id+1))\n",
        "        x = cv2.imread(x_path, cv2.IMREAD_COLOR)\n",
        "        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "        if np.random.rand(1) < config.gray_ratio: # data augmentation for gray image to color image\n",
        "            z = cv2.cvtColor(z, cv2.COLOR_RGB2GRAY)\n",
        "            z = cv2.cvtColor(z, cv2.COLOR_GRAY2RGB)\n",
        "            x = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
        "            x = cv2.cvtColor(x, cv2.COLOR_GRAY2RGB)\n",
        "        z = self.z_transforms(z)\n",
        "        x = self.x_transforms(x)\n",
        "        return z, x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdgLbVww5ZWp"
      },
      "source": [
        "# import torch\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "\n",
        "class CenterCrop(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        shape = sample.shape[:2]\n",
        "        cy, cx = (shape[0]-1) // 2, (shape[1]-1) // 2\n",
        "        ymin, xmin = cy - self.size[0]//2, cx - self.size[1] // 2\n",
        "        ymax, xmax = cy + self.size[0]//2 + self.size[0] % 2,\\\n",
        "                     cx + self.size[1]//2 + self.size[1] % 2\n",
        "        left = right = top = bottom = 0\n",
        "        im_h, im_w = shape\n",
        "        if xmin < 0:\n",
        "            left = int(abs(xmin))\n",
        "        if xmax > im_w:\n",
        "            right = int(xmax - im_w)\n",
        "        if ymin < 0:\n",
        "            top = int(abs(ymin))\n",
        "        if ymax > im_h:\n",
        "            bottom = int(ymax - im_h)\n",
        "\n",
        "        xmin = int(max(0, xmin))\n",
        "        xmax = int(min(im_w, xmax))\n",
        "        ymin = int(max(0, ymin))\n",
        "        ymax = int(min(im_h, ymax))\n",
        "        im_patch = sample[ymin:ymax, xmin:xmax]\n",
        "        if left != 0 or right !=0 or top!=0 or bottom!=0:\n",
        "            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n",
        "                    cv2.BORDER_CONSTANT, value=0)\n",
        "        return im_patch\n",
        "\n",
        "class RandomCrop(object):\n",
        "    def __init__(self, size, max_translate):\n",
        "        self.size = size\n",
        "        self.max_translate = max_translate\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        shape = sample.shape[:2]\n",
        "        cy_o = (shape[0] - 1) // 2\n",
        "        cx_o = (shape[1] - 1) // 2\n",
        "        cy = np.random.randint(cy_o - self.max_translate, \n",
        "                               cy_o + self.max_translate+1)\n",
        "        cx = np.random.randint(cx_o - self.max_translate,\n",
        "                               cx_o + self.max_translate+1)\n",
        "        assert abs(cy-cy_o) <= self.max_translate and \\\n",
        "                abs(cx-cx_o) <= self.max_translate\n",
        "        ymin = cy - self.size[0] // 2\n",
        "        xmin = cx - self.size[1] // 2\n",
        "        ymax = cy + self.size[0] // 2 + self.size[0] % 2\n",
        "        xmax = cx + self.size[1] // 2 + self.size[1] % 2\n",
        "        left = right = top = bottom = 0\n",
        "        im_h, im_w = shape\n",
        "        if xmin < 0:\n",
        "            left = int(abs(xmin))\n",
        "        if xmax > im_w:\n",
        "            right = int(xmax - im_w)\n",
        "        if ymin < 0:\n",
        "            top = int(abs(ymin))\n",
        "        if ymax > im_h:\n",
        "            bottom = int(ymax - im_h)\n",
        "\n",
        "        xmin = int(max(0, xmin))\n",
        "        xmax = int(min(im_w, xmax))\n",
        "        ymin = int(max(0, ymin))\n",
        "        ymax = int(min(im_h, ymax))\n",
        "        im_patch = sample[ymin:ymax, xmin:xmax]\n",
        "        if left != 0 or right !=0 or top!=0 or bottom!=0:\n",
        "            im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n",
        "                    cv2.BORDER_CONSTANT, value=0)\n",
        "        return im_patch\n",
        "\n",
        "class RandomStretch(object):\n",
        "    def __init__(self, max_stretch=0.05):\n",
        "        self.max_stretch = max_stretch\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        scale_h = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n",
        "        scale_w = 1.0 + np.random.uniform(-self.max_stretch, self.max_stretch)\n",
        "        h, w = sample.shape[:2]\n",
        "        shape = (int(h * scale_h), int(w * scale_w))\n",
        "        return cv2.resize(sample, shape, cv2.INTER_LINEAR)\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        sample = sample.transpose(2, 0, 1)\n",
        "        return torch.from_numpy(sample.astype(np.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6i5Jp545gZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61a4bee8-bec9-4ca1-ae41-81a0f1de73b0"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from tensorboardX import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "train_data_dir = '/content/drive/MyDrive/Colab Notebooks/train_crop'\n",
        "val_data_dir = '/content/drive/MyDrive/Colab Notebooks/val_crop'\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    train_z_transforms = transforms.Compose([\n",
        "        RandomStretch(),\n",
        "        CenterCrop((config.exemplar_size, config.exemplar_size)),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    train_x_transforms = transforms.Compose([\n",
        "        RandomStretch(),\n",
        "        RandomCrop((config.instance_size, config.instance_size),\n",
        "                   config.max_translate),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    val_z_transforms = transforms.Compose([\n",
        "        CenterCrop((config.exemplar_size, config.exemplar_size)),\n",
        "        ToTensor()\n",
        "    ])\n",
        "    val_x_transforms = transforms.Compose([\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "    # create training dataset & validation dataset\n",
        "    train_dataset = Dataset_GOT(train_data_dir, train_z_transforms, train_x_transforms)\n",
        "    valid_dataset = Dataset_GOT(val_data_dir, val_z_transforms, val_x_transforms, training=False)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=config.train_batch_size,\n",
        "                             shuffle=True, pin_memory=True, num_workers=config.train_num_workers, drop_last=True)\n",
        "    validloader = DataLoader(valid_dataset, batch_size=config.valid_batch_size,\n",
        "                             shuffle=False, pin_memory=True, num_workers=config.valid_num_workers, drop_last=True)\n",
        "\n",
        "    # training-start\n",
        "    with torch.cuda.device(0):\n",
        "        total_train_loss = []\n",
        "        total_val_loss = []\n",
        "        model = SiamFCNet()\n",
        "        model.init_weights()\n",
        "        model = model.cuda()\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
        "        schdeuler = StepLR(optimizer, step_size=config.step_size,\n",
        "                           gamma=config.gamma)\n",
        "\n",
        "        for epoch in range(config.epoch):\n",
        "            train_loss = []\n",
        "            model.train()\n",
        "            for i, data in enumerate(tqdm(trainloader)):\n",
        "                z, x = data\n",
        "                z, x = Variable(z.cuda()), Variable(x.cuda())\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(z, x)\n",
        "                loss = model.weighted_loss(outputs)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                step = epoch * len(trainloader) + i\n",
        "                train_loss.append(loss.data.cpu().numpy())\n",
        "            train_loss = np.mean(train_loss)\n",
        "            total_train_loss.append(train_loss)\n",
        "            valid_loss = []\n",
        "            model.eval()\n",
        "            for i, data in enumerate(tqdm(validloader)):\n",
        "                z, x = data\n",
        "                z, x = Variable(z.cuda()), Variable(x.cuda())\n",
        "                outputs = model(z, x)\n",
        "                loss = model.weighted_loss(outputs)\n",
        "                valid_loss.append(loss.data.cpu().numpy())\n",
        "            valid_loss = np.mean(valid_loss)\n",
        "            total_val_loss.append(valid_loss)\n",
        "            print(\"EPOCH %d valid_loss: %.4f, train_loss: %.4f, learning_rate: %.4f\" %\n",
        "                  (epoch, valid_loss, train_loss, optimizer.param_groups[0][\"lr\"]))\n",
        "            torch.save(model.cpu().state_dict(),\n",
        "                       \"/content/drive/MyDrive/Colab Notebooks/model_new3/siamfc_{}.pth\".format(epoch + 1))\n",
        "            model.cuda()\n",
        "            schdeuler.step()\n",
        "\n",
        "        plt.plot(total_train_loss, label = \"Training loss\")\n",
        "        plt.plot(total_val_loss, label = \"Testing loss\")\n",
        "        plt.legend()\n",
        "        print('')\n",
        "        print('')\n",
        "        print('')\n",
        "        print('')\n",
        "        print('total_train_loss', total_train_loss)\n",
        "        print('total_val_loss', total_val_loss)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 610/610 [01:37<00:00,  6.23it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0 valid_loss: 0.3154, train_loss: 0.3479, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1 valid_loss: 0.2898, train_loss: 0.2317, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.22it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 2 valid_loss: 0.2843, train_loss: 0.1951, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 3 valid_loss: 0.3173, train_loss: 0.1741, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 4 valid_loss: 0.3075, train_loss: 0.1574, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:37<00:00,  6.22it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 5 valid_loss: 0.3122, train_loss: 0.1495, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 6 valid_loss: 0.3248, train_loss: 0.1379, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:37<00:00,  6.23it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 7 valid_loss: 0.3223, train_loss: 0.1315, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.22it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 8 valid_loss: 0.2973, train_loss: 0.1248, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.20it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 9 valid_loss: 0.3688, train_loss: 0.1216, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.22it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 10 valid_loss: 0.3179, train_loss: 0.1144, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.18it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 11 valid_loss: 0.4056, train_loss: 0.1138, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 12 valid_loss: 0.2819, train_loss: 0.1097, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 13 valid_loss: 0.3575, train_loss: 0.1039, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 14 valid_loss: 0.4588, train_loss: 0.1034, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 15 valid_loss: 0.2722, train_loss: 0.1008, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.22it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 16 valid_loss: 0.3067, train_loss: 0.0966, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 17 valid_loss: 0.2946, train_loss: 0.0940, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 18 valid_loss: 0.3843, train_loss: 0.0955, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 19 valid_loss: 0.3065, train_loss: 0.0942, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 20 valid_loss: 0.3065, train_loss: 0.0911, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.17it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 21 valid_loss: 0.3024, train_loss: 0.0865, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.17it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 22 valid_loss: 0.3220, train_loss: 0.0856, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.18it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 23 valid_loss: 0.2904, train_loss: 0.0860, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 24 valid_loss: 0.3115, train_loss: 0.0855, learning_rate: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.20it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 25 valid_loss: 0.2811, train_loss: 0.0757, learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.18it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 26 valid_loss: 0.2662, train_loss: 0.0721, learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.18it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 27 valid_loss: 0.2888, train_loss: 0.0707, learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.19it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 28 valid_loss: 0.2825, train_loss: 0.0698, learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 610/610 [01:38<00:00,  6.21it/s]\n",
            "100%|██████████| 65/65 [00:06<00:00, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 29 valid_loss: 0.3032, train_loss: 0.0695, learning_rate: 0.0010\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "total_train_loss [0.34789103, 0.23174313, 0.19510631, 0.17406827, 0.15736702, 0.14949118, 0.13787931, 0.13151978, 0.12476426, 0.121572785, 0.11435379, 0.11384258, 0.10967743, 0.10394391, 0.103399664, 0.100762844, 0.09658687, 0.09396016, 0.09545533, 0.094175845, 0.09110993, 0.08647476, 0.08556662, 0.08596877, 0.08549381, 0.07567437, 0.07212202, 0.07071772, 0.069846354, 0.0694942]\n",
            "total_val_loss [0.31541908, 0.28976026, 0.28434727, 0.317297, 0.30752215, 0.31224054, 0.32481217, 0.32228473, 0.2973034, 0.3687703, 0.31790432, 0.40560472, 0.2818659, 0.35750666, 0.4588119, 0.2722092, 0.30671728, 0.29464102, 0.3843274, 0.30651546, 0.30648914, 0.30235913, 0.32199302, 0.29039267, 0.31145346, 0.28107348, 0.26619515, 0.28875953, 0.282545, 0.3031915]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQvi8Yhk8nB4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def region_to_bbox(region, center=True):\n",
        "\n",
        "    n = len(region)\n",
        "    assert n==4 or n==8, ('GT region format is invalid, should have 4 or 8 entries.')\n",
        "\n",
        "    if n==4:\n",
        "        return _rect(region, center)\n",
        "    else:\n",
        "        return _poly(region, center)\n",
        "\n",
        "# we assume the grountruth bounding boxes are saved with 0-indexing\n",
        "def _rect(region, center):\n",
        "    \n",
        "    if center:\n",
        "        x = region[0]\n",
        "        y = region[1]\n",
        "        w = region[2]\n",
        "        h = region[3]\n",
        "        cx = x+w/2\n",
        "        cy = y+h/2\n",
        "        return cx, cy, w, h\n",
        "    else:\n",
        "        #region[0] -= 1\n",
        "        #region[1] -= 1\n",
        "        return region\n",
        "\n",
        "\n",
        "def _poly(region, center):\n",
        "    cx = np.mean(region[::2])\n",
        "    cy = np.mean(region[1::2])\n",
        "    x1 = np.min(region[::2])\n",
        "    x2 = np.max(region[::2])\n",
        "    y1 = np.min(region[1::2])\n",
        "    y2 = np.max(region[1::2])\n",
        "    A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n",
        "    A2 = (x2 - x1) * (y2 - y1)\n",
        "    s = np.sqrt(A1/A2)\n",
        "    w = s * (x2 - x1) + 1\n",
        "    h = s * (y2 - y1) + 1\n",
        "\n",
        "    if center:\n",
        "        return cx, cy, w, h\n",
        "    else:\n",
        "        return cx-w/2, cy-h/2, w, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZqzndRqNMpg"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import warnings\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.set_num_threads(1) # otherwise pytorch will take all cpus\n",
        "\n",
        "class SiamFCTracker:\n",
        "    def __init__(self, model_path, gpu_id):\n",
        "        self.gpu_id = gpu_id\n",
        "        with torch.cuda.device(gpu_id):\n",
        "            self.model = SiamFCNet(training=False)\n",
        "            self.model.load_state_dict(torch.load(model_path))\n",
        "            self.model = self.model.cuda()\n",
        "            self.model.eval() \n",
        "        self.transforms = transforms.Compose([\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "    def _cosine_window(self, size):\n",
        "        \"\"\"\n",
        "            focuses on the central region of target and weakens the background information near the bounding box edge\n",
        "        \"\"\"\n",
        "        cos_window = np.hanning(int(size[0]))[:, np.newaxis].dot(np.hanning(int(size[1]))[np.newaxis, :])\n",
        "        cos_window = cos_window.astype(np.float32)\n",
        "        cos_window /= np.sum(cos_window)\n",
        "        return cos_window\n",
        "\n",
        "    def init(self, frame, bbox):\n",
        "        # bbox: [x, y, width, height]\n",
        "        self.bbox = (bbox[0]-1, bbox[1]-1, bbox[0]-1+bbox[2], bbox[1]-1+bbox[3]) # zero based\n",
        "        self.pos = np.array([bbox[0]-1+(bbox[2]-1)/2, bbox[1]-1+(bbox[3]-1)/2])  # center x, center y, zero based\n",
        "        self.target_sz = np.array([bbox[2], bbox[3]])                            # width, height\n",
        "        # get exemplar img\n",
        "        self.img_mean = tuple(map(int, frame.mean(axis=(0, 1)))) # Calculate mean color\n",
        "        exemplar_img, scale_z, s_z = get_exemplar_image(frame, self.bbox,\n",
        "                config.exemplar_size, config.context_amount, self.img_mean)\n",
        "\n",
        "        # get exemplar feature\n",
        "        exemplar_img = self.transforms(exemplar_img)[None,:,:,:] #(127, 127, 3) to (3, 127, 127)\n",
        "\n",
        "        with torch.cuda.device(self.gpu_id):\n",
        "            exemplar_img_var = Variable(exemplar_img.cuda())\n",
        "            self.model(exemplar_img_var, None)\n",
        "\n",
        "        self.penalty = np.ones((config.num_scale)) * config.scale_penalty\n",
        "        self.penalty[config.num_scale//2] = 1 #[0.9745 1.     0.9745]\n",
        "\n",
        "        # create cosine window\n",
        "        \"\"\"upsampling score map for more accurate localization\n",
        "           from window & interpolation, can reduce the effect due to spectral leakage in sampling process\n",
        "        \"\"\"\n",
        "        self.interp_response_sz = config.response_up_stride * config.response_sz #bicubic interpolation, from 17 × 17 to 272 × 272 - accurate localization\n",
        "        self.cosine_window = self._cosine_window((self.interp_response_sz, self.interp_response_sz))\n",
        "\n",
        "        # create scalse\n",
        "        \"\"\" handle scale variations, we also search for the object over three scales. [-1, 0, 1] \"\"\"\n",
        "        self.scales = config.scale_step ** np.arange(np.ceil(config.num_scale/2)-config.num_scale,\n",
        "                np.floor(config.num_scale/2)+1)\n",
        "\n",
        "        # create s_x\n",
        "        self.s_x = s_z + (config.instance_size-config.exemplar_size) / scale_z\n",
        "\n",
        "        # arbitrary scale saturation\n",
        "        self.min_s_x = 0.2 * self.s_x \n",
        "        self.max_s_x = 5 * self.s_x\n",
        "\n",
        "    def update(self, frame):\n",
        "        # make three scales\n",
        "        size_x_scales = self.s_x * self.scales\n",
        "        # make three different scaled image. shape (255, 255, 3) \n",
        "        pyramid = get_scaled_instance_image(frame, self.pos, config.instance_size, size_x_scales, self.img_mean)\n",
        "        instance_imgs = torch.cat([self.transforms(x)[None,:,:,:] for x in pyramid], dim=0)\n",
        "\n",
        "        with torch.cuda.device(self.gpu_id):\n",
        "            instance_imgs_var = Variable(instance_imgs.cuda())\n",
        "            response_maps = self.model(None, instance_imgs_var)\n",
        "            response_maps = response_maps.data.cpu().numpy().squeeze() # (3, 17, 17)\n",
        "            #bicubic interpolation over square pixel neighborhoods\n",
        "            response_maps_up = [cv2.resize(x, (self.interp_response_sz, self.interp_response_sz), cv2.INTER_CUBIC)\n",
        "             for x in response_maps]\n",
        "        # get max score\n",
        "        max_score = np.array([x.max() for x in response_maps_up]) * self.penalty\n",
        "\n",
        "        # penalty scale change\n",
        "        scale_idx = max_score.argmax() #index which have max score\n",
        "        response_map = response_maps_up[scale_idx]\n",
        "        response_map -= response_map.min()\n",
        "        response_map /= response_map.sum()\n",
        "        response_map = (1 - config.window_influence) * response_map + \\\n",
        "                config.window_influence * self.cosine_window\n",
        "        max_r, max_c = np.unravel_index(response_map.argmax(), response_map.shape)\n",
        "\n",
        "        # displacement in interpolation response\n",
        "        disp_response_interp = np.array([max_c, max_r]) - (self.interp_response_sz-1) / 2.\n",
        "        # displacement in input\n",
        "        disp_response_input = disp_response_interp * config.total_stride / config.response_up_stride\n",
        "        # displacement in frame\n",
        "        scale = self.scales[scale_idx]\n",
        "        disp_response_frame = disp_response_input * (self.s_x * scale) / config.instance_size\n",
        "        # position in frame coordinates\n",
        "        self.pos += disp_response_frame\n",
        "        # scale damping and saturation\n",
        "        self.s_x *= ((1 - config.scale_lr) + config.scale_lr * scale)\n",
        "        self.s_x = max(self.min_s_x, min(self.max_s_x, self.s_x))\n",
        "        self.target_sz = ((1 - config.scale_lr) + config.scale_lr * scale) * self.target_sz\n",
        "        bbox = (self.pos[0] - self.target_sz[0]/2 + 1, # xmin   convert to 1-based\n",
        "                self.pos[1] - self.target_sz[1]/2 + 1, # ymin\n",
        "                self.pos[0] + self.target_sz[0]/2 + 1, # xmax\n",
        "                self.pos[1] + self.target_sz[1]/2 + 1) # ymax\n",
        "        return bbox #1-based bounding box(xmin, ymin, xmax, ymax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOS4XWRpT8P8"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def get_center(x):\n",
        "    return (x - 1.) / 2.\n",
        "\n",
        "def xyxy2cxcywh(bbox):\n",
        "    return get_center(bbox[0]+bbox[2]), \\\n",
        "           get_center(bbox[1]+bbox[3]), \\\n",
        "           (bbox[2]-bbox[0]), \\\n",
        "           (bbox[3]-bbox[1])\n",
        "\n",
        "def crop_and_pad(img, cx, cy, model_sz, original_sz, img_mean=None):\n",
        "    xmin = cx - original_sz // 2\n",
        "    xmax = cx + original_sz // 2\n",
        "    ymin = cy - original_sz // 2\n",
        "    ymax = cy + original_sz // 2\n",
        "    im_h, im_w, _ = img.shape\n",
        "\n",
        "    left = right = top = bottom = 0\n",
        "    if xmin < 0:\n",
        "        left = int(abs(xmin))\n",
        "    if xmax > im_w:\n",
        "        right = int(xmax - im_w)\n",
        "    if ymin < 0:\n",
        "        top = int(abs(ymin))\n",
        "    if ymax > im_h:\n",
        "        bottom = int(ymax - im_h)\n",
        "\n",
        "    xmin = int(max(0, xmin))\n",
        "    xmax = int(min(im_w, xmax))\n",
        "    ymin = int(max(0, ymin))\n",
        "    ymax = int(min(im_h, ymax))\n",
        "    im_patch = img[ymin:ymax, xmin:xmax]\n",
        "    if left != 0 or right !=0 or top!=0 or bottom!=0:\n",
        "        if img_mean is None:\n",
        "            img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n",
        "        im_patch = cv2.copyMakeBorder(im_patch, top, bottom, left, right,\n",
        "                cv2.BORDER_CONSTANT, value=img_mean)\n",
        "    if model_sz != original_sz:\n",
        "        im_patch = cv2.resize(im_patch, (model_sz, model_sz))\n",
        "    return im_patch\n",
        "\n",
        "def get_exemplar_image(img, bbox, size_z, context_amount, img_mean=None):\n",
        "    # xyxy to cxcywh convertion\n",
        "    cx, cy, w, h = xyxy2cxcywh(bbox)\n",
        "    # s(w + 2p) × s(h + 2p) = exemplar_size\n",
        "    wc_z = w + context_amount * (w+h)\n",
        "    hc_z = h + context_amount * (w+h)\n",
        "    s_z = np.sqrt(wc_z * hc_z)\n",
        "    scale_z = size_z / s_z\n",
        "    exemplar_img = crop_and_pad(img, cx, cy, size_z, s_z, img_mean)\n",
        "    return exemplar_img, scale_z, s_z\n",
        "\n",
        "def get_scaled_instance_image(img, center, size_x, size_x_scales, img_mean=None):\n",
        "    if img_mean is None:\n",
        "        img_mean = tuple(map(int, img.mean(axis=(0, 1))))\n",
        "    pyramid = [crop_and_pad(img, center[0], center[1], size_x, size_x_scale, img_mean)\n",
        "            for size_x_scale in size_x_scales]\n",
        "    return pyramid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VTDezSwxS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41bc68f-76b7-4b24-ba76-ce187a1fc351"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/test4'\n",
        "model_dir = '/content/drive/MyDrive/Colab Notebooks/model_new3/siamfc_30.pth'\n",
        "outputpath = '/content/drive/MyDrive/Colab Notebooks/video7'\n",
        "\n",
        "def _init_video(datapath, video):\n",
        "\n",
        "    frame_name_list = glob.glob(os.path.join(datapath, video)+'/**.jpg')\n",
        "    frame_name_list.sort()\n",
        "\n",
        "    try:\n",
        "        gt_file = os.path.join(datapath, video, 'groundtruth.txt')\n",
        "        try:\n",
        "            gt = np.loadtxt(gt_file, dtype=float, delimiter=',')\n",
        "        except:\n",
        "            gt = np.loadtxt(gt_file, dtype=float)\n",
        "    except:\n",
        "        gt_file = os.path.join(datapath, video, 'groundtruth_rect.txt')\n",
        "        try:\n",
        "            gt = np.loadtxt(gt_file, dtype=float, delimiter=',')\n",
        "        except:\n",
        "            gt = np.loadtxt(gt_file, dtype=float)\n",
        "\n",
        "    n_frames = len(frame_name_list)\n",
        "\n",
        "    # count of frames eqal to length of ground truth\n",
        "    # assert n_frames == len(gt)\n",
        "    return gt, frame_name_list, n_frames\n",
        "\n",
        "\n",
        "def run_SiamFC():\n",
        "\n",
        "\n",
        "    videolist = os.listdir(data_dir)\n",
        "    videolist.sort()\n",
        "    video_count = len(videolist)\n",
        "    precision_all = 0\n",
        "    iou_all = 0\n",
        "    fps_all = 0\n",
        "\n",
        "    for video in videolist:\n",
        "        gt, frame_name_list, n_frames = _init_video(data_dir, video)\n",
        "        bboxs = np.zeros((n_frames, 4)) # create matrix to save all bbox values\n",
        "        path_cor = [] # create matrix to save all path coordinates\n",
        "        tracker = SiamFCTracker(model_dir, config.gpu_id)\n",
        "        \n",
        "        start = time.time()\n",
        "        for i in range(n_frames):\n",
        "            # Open CV use BGR color. Convert BGR to RGB\n",
        "            frame = cv2.imread(frame_name_list[i], cv2.IMREAD_COLOR)\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "            if i==0:\n",
        "                tracker.init(frame, gt)\n",
        "                bbox = gt\n",
        "                bbox = (bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3])\n",
        "            else:\n",
        "                bbox = tracker.update(frame)\n",
        "\n",
        "            bboxs[i] = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
        "            path_cor.append([int((bbox[2] + bbox[0])/2), int(bbox[3]+ ((bbox[1]-bbox[3])/10))])\n",
        "\n",
        "            pts = np.array(path_cor, np.int32)\n",
        "            pts = pts.reshape((-1, 1, 2))\n",
        "            cv2.polylines(frame, [pts], isClosed = False,color = (0,255,0),thickness = 3)\n",
        "\n",
        "            # visualization\n",
        "            cv2.rectangle(frame,(int(bbox[0]),int(bbox[1])),(int(bbox[2]),int(bbox[3])),(255,0,0),thickness=2)\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n",
        "            # cv2_imshow(frame)\n",
        "            savepath = os.path.join(outputpath, video)\n",
        "            if not os.path.exists(savepath):\n",
        "                os.mkdir(savepath)\n",
        "            # path = savepath+str(i)+'new.jpg'\n",
        "            cv2.imwrite(savepath+\"/{:0>8d}.jpg\".format(i+1), frame)\n",
        "\n",
        "            end = time.time()\n",
        "            fps = 1/ ((end-start)/n_frames)\n",
        "        _, precision, _, iou = compile_results(gt, bboxs, 20)\n",
        "        print(\"video \" + ' -- ' + str(video) + \"  Precision: \" + str(precision) + \"  IOU: \" + str(iou) + \"  FPS: \" + str(fps))\n",
        "        precision_all = precision_all + precision\n",
        "        iou_all = iou_all + iou\n",
        "        fps_all = fps_all + fps\n",
        "\n",
        "    P = precision_all / video_count\n",
        "    I = iou_all / video_count\n",
        "    F = fps_all / video_count\n",
        "    print(\"averge_Precision:\" + str(P))\n",
        "    print(\"averge_IOU:\" + str(I))\n",
        "    print(\"averge_FPS:\" + str(F))\n",
        "    # out.release()\n",
        "\n",
        "\n",
        "def compile_results(gt, bboxes, dist_threshold):\n",
        "    l = np.size(bboxes, 0)\n",
        "    gt4 = np.zeros((l, 4))\n",
        "    new_distances = np.zeros(l)\n",
        "    new_ious = np.zeros(l)\n",
        "    n_thresholds = 50\n",
        "    precisions_ths = np.zeros(n_thresholds)\n",
        "\n",
        "    for i in range(l):\n",
        "        gt4[i, :] = region_to_bbox(gt, center=False)\n",
        "        new_distances[i] = compute_distance(bboxes[i, :], gt4[i, :])\n",
        "        new_ious[i] = compute_iou(bboxes[i, :], gt4[i, :])\n",
        "\n",
        "    # what's the percentage of frame in which center displacement is inferior to given threshold? (OTB metric)\n",
        "    precision = sum(new_distances < dist_threshold)/np.size(new_distances) * 100\n",
        "\n",
        "    # find above result for many thresholds, then report the AUC\n",
        "    thresholds = np.linspace(0, 25, n_thresholds+1)\n",
        "    thresholds = thresholds[-n_thresholds:]\n",
        "    # reverse it so that higher values of precision goes at the beginning\n",
        "    thresholds = thresholds[::-1]\n",
        "    for i in range(n_thresholds):\n",
        "        precisions_ths[i] = sum(new_distances < thresholds[i])/np.size(new_distances)\n",
        "\n",
        "    # integrate over the thresholds\n",
        "    precision_auc = np.trapz(precisions_ths)\n",
        "\n",
        "    # per frame averaged intersection over union (OTB metric)\n",
        "    iou = np.mean(new_ious) * 100\n",
        "\n",
        "    return l, precision, precision_auc, iou\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
        "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
        "\n",
        "    if xA < xB and yA < yB:\n",
        "        # compute the area of intersection rectangle\n",
        "        interArea = (xB - xA) * (yB - yA)\n",
        "        # compute the area of both the prediction and ground-truth\n",
        "        # rectangles\n",
        "        boxAArea = boxA[2] * boxA[3]\n",
        "        boxBArea = boxB[2] * boxB[3]\n",
        "        # compute the intersection over union by taking the intersection\n",
        "        # area and dividing it by the sum of prediction + ground-truth\n",
        "        # areas - the intersection area\n",
        "        iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    else:\n",
        "        iou = 0\n",
        "\n",
        "    assert iou >= 0\n",
        "    assert iou <= 1.01\n",
        "    return iou\n",
        "\n",
        "def compute_distance(boxA, boxB):\n",
        "    a = np.array((boxA[0]+boxA[2]/2, boxA[1]+boxA[3]/2))\n",
        "    b = np.array((boxB[0]+boxB[2]/2, boxB[1]+boxB[3]/2))\n",
        "    dist = np.linalg.norm(a - b)\n",
        "\n",
        "    assert dist >= 0\n",
        "    assert dist != float('Inf')\n",
        "    return dist\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_SiamFC()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video  -- GOT-10k_Val_000039 (1)  Precision: 12.222222222222221  IOU: 23.415356571962384  FPS: 13.272626234373595\n",
            "averge_Precision:12.222222222222221\n",
            "averge_IOU:23.415356571962384\n",
            "averge_FPS:13.272626234373595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXhKspjlhfKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3de354-3235-4e81-d338-e8e67165786f"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121'\n",
        "\n",
        "frame_name_list = glob.glob(os.path.join(data_dir)+'/**.jpg')\n",
        "print(frame_name_list)\n",
        "\n",
        "def write(frames, show=False):\n",
        "        out = None\n",
        "\n",
        "        try:\n",
        "            for i in range(len(frames)):\n",
        "                frame = cv2.imread(frames[i], cv2.IMREAD_COLOR)\n",
        "                if show:\n",
        "                    cv2.imshow('video', frame)\n",
        "\n",
        "                if not out:\n",
        "                    height, width, channels = frame.shape\n",
        "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "                    out = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/test4/example22.mp4', fourcc, 12, (width, height))\n",
        "\n",
        "                out.write(frame)\n",
        "\n",
        "        finally:\n",
        "            out and out.release()\n",
        "            cv2.destroyAllWindows() \n",
        "\n",
        "write(frame_name_list, show=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000001.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000002.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000003.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000004.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000005.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000006.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000007.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000008.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000009.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000010.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000011.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000012.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000013.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000014.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000015.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000016.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000017.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000018.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000019.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000020.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000021.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000022.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000023.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000024.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000025.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000026.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000027.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000028.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000029.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000030.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000031.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000032.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000033.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000034.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000035.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000036.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000037.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000038.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000039.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000040.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000041.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000042.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000043.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000044.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000045.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000046.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000047.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000048.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000049.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000050.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000051.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000052.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000053.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000054.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000055.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000056.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000057.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000058.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000059.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000060.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000061.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000062.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000063.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000064.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000065.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000066.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000067.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000068.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000069.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000070.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000071.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000072.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000073.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000074.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000075.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000076.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000077.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000078.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000079.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000080.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000081.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000082.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000083.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000084.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000085.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000086.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000087.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000088.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000089.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000090.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000091.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000092.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000093.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000094.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000095.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000096.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000097.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000098.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000099.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000100.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000101.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000102.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000103.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000104.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000105.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000106.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000107.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000108.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000109.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000110.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000111.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000112.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000113.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000114.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000115.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000116.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000117.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000118.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000119.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000120.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000121.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000122.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000123.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000124.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000125.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000126.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000127.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000128.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000129.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000130.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000131.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000132.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000133.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000134.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000135.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000136.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000137.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000138.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000139.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000140.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000141.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000142.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000143.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000144.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000145.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000146.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000147.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000148.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000149.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000150.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000151.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000152.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000153.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000154.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000155.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000156.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000157.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000158.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000159.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000160.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000161.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000162.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000163.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000164.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000165.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000166.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000167.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000168.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000169.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000170.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000171.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000172.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000173.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000174.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000175.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000176.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000177.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000178.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000179.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000180.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000181.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000182.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000183.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000184.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000185.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000186.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000187.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000188.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000189.jpg', '/content/drive/MyDrive/Colab Notebooks/video7/GOT-10k_Test_000121/00000190.jpg']\n"
          ]
        }
      ]
    }
  ]
}